{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic python and ML Libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "# reading images using OpenCV\n",
    "import cv2\n",
    "\n",
    "# matplotlib & others for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "# torchvision libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# Miscellaneous\n",
    "from torchinfo import summary\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer\n",
    "from typing import List, Union\n",
    "\n",
    "# for images\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "root = pathlib.Path(\"data\") / \"obj_detection_tutorial\" / 'data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def show_bbox(image_path, label_path):\n",
    "    image = Image.open(image_path)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    with open(label_path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            label, x, y, w, h = line.split(' ')\n",
    "            x = float(x)\n",
    "            y = float(y)\n",
    "            w = float(w)\n",
    "            h = float(h)\n",
    "\n",
    "            W, H = image.size\n",
    "            x1 = (x - w/2) * W\n",
    "            y1 = (y - h/2) * H\n",
    "            x2 = (x + w/2) * W\n",
    "            y2 = (y + h/2) * H\n",
    "            \n",
    "            draw.rectangle((x1,y1,x2,y2), outline = (255,0,0), width = 5)\n",
    "    image.show()\n",
    "\n",
    "def tensorprint(tensor):\n",
    "    print(\"Shape: \" , tensor.shape, \" , Dimension: \", tensor.ndim , \" \\nDtype: \", tensor.dtype, \" , Device: \", tensor.device)\n",
    "    print(\"Max: \", tensor.amax(),f'[{tensor.argmax()}]', \" , Min: \", tensor.amin(),f'[{tensor.argmin()}]')\n",
    "    #print(tensor ,'\\n')\n",
    "\n",
    "def torch_rng():\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def OD_train_step(model, optimizer, data_loader, device = device):\n",
    "    model.train()\n",
    "    train_loss_list = []\n",
    "    c = 0\n",
    "\n",
    "    for images , targets in data_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        train_loss_list.append(loss_value)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        c+=1\n",
    "\n",
    "        if c % 4 == 0:\n",
    "            print(f\"Training loss: {loss_value}\")\n",
    "\n",
    "    return train_loss_list\n",
    "\n",
    "def OD_test_step(model, optimizer, data_loader, device = device):\n",
    "    val_loss_list = []\n",
    "    c= 0\n",
    "\n",
    "    for images, targets in data_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            loss_dict = model(images, targets)\n",
    "            \n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            loss_value = losses.item()\n",
    "            val_loss_list.append(loss_value)\n",
    "            c+=1\n",
    "\n",
    "            if c % 4 == 0:\n",
    "                print(f\"Testing loss: {loss_value}\")\n",
    "\n",
    "        return val_loss_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show image\n",
    "# image_p = train_dir / '1.jpg'\n",
    "# label_p = root / 'labels' / 'train' / '1.txt'\n",
    "#show_bbox(image_p, label_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "train_dir = root / 'Images' / \"train\"\n",
    "test_dir = root / 'Images' / 'test'\n",
    "val_dir = root / 'Images' / 'val'\n",
    "train_label_dir = root /'labels' / 'train'\n",
    "test_label_dir = root /'labels' / 'test'\n",
    "val_label_dir = root /'labels' / 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset \n",
    "\n",
    "class ConeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image, label , transforms) -> None:\n",
    "        self.image = image\n",
    "        self.labelp = label\n",
    "        self.transforms = transforms\n",
    "        self.imgs = list(sorted(os.listdir(image)))\n",
    "        self.label = list(sorted(os.listdir(label)))\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        W, H =  224,224\n",
    "        img_path = os.path.join(self.image , self.imgs[idx])\n",
    "        label_path = os.path.join(self.labelp, self.label[idx])\n",
    "        img = Image.open(img_path)\n",
    "        img = img.resize((W,H), Image.ANTIALIAS)\n",
    "        img = img.convert(\"RGB\")\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "    \n",
    "        with open(label_path) as file:\n",
    "            for line in file:\n",
    "                labels.append(1)\n",
    "\n",
    "                parsed = [float(x) for x in line.split(' ')]\n",
    "                x_center = parsed[1]\n",
    "                y_center = parsed[2]\n",
    "                box_wt = parsed[3]\n",
    "                box_ht = parsed[4]\n",
    "\n",
    "                xmin = int((x_center - box_wt/2)*W)\n",
    "                xmax = int((x_center + box_wt/2)*W)\n",
    "                ymin = int((y_center - box_ht/2)*H)\n",
    "                ymax = int((y_center + box_ht/2)*H)\n",
    "\n",
    "                boxes.append([xmin,ymin,xmax,ymax])\n",
    "        \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels,dtype=torch.int64)\n",
    "        area = ((boxes[:,3] - boxes[:,1]) * (boxes[:, 2] - boxes[:, 0]))\n",
    "        iscrowd = torch.zeros(boxes.shape[0], dtype = torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img , target = self.transforms(img, target)\n",
    "        \n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "        \n",
    "\n",
    "e = ConeDataset(train_dir, train_label_dir, None)\n",
    "e[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloading \n",
    "BATCH_SIZE = 4\n",
    "\n",
    "def get_transform(img, target):\n",
    "    train_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),                        \n",
    "    ])\n",
    "\n",
    "    return train_transform(img), target\n",
    "\n",
    "\n",
    "train_data = ConeDataset(train_dir, train_label_dir,get_transform)\n",
    "test_data = ConeDataset(test_dir, test_label_dir,get_transform)                                \n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_data, \n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                shuffle=True,\n",
    "                                collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                shuffle=False,\n",
    "                                collate_fn=collate_fn)\n",
    "                                \n",
    "img, target = next(iter(test_dataloader))\n",
    "images = list(image.to(device) for image in img)\n",
    "targets = [{k : v.to(device) for k , v in t.items()} for t in target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model_6 = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "in_features = model_6.roi_heads.box_predictor.cls_score.in_features\n",
    "num_classes = 2\n",
    "model_6.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "model_6.to(device)\n",
    "model_6(images, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "params = [p for p in model_6.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=3,gamma=0.1)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    train_results = OD_train_step(model_6,optimizer,train_dataloader)\n",
    "    lr_scheduler.step()\n",
    "    test_results  = OD_test_step(model_6,optimizer,test_dataloader)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "val = os.listdir(val_dir)\n",
    "\n",
    "for i in range(len(val)):\n",
    "    joinedp = os.path.join(val_dir, val[i])\n",
    "    image = cv2.imread(joinedp)\n",
    "    orig_image = image.copy()\n",
    "    image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "    image /= 255.0\n",
    "    image = np.transpose(image, (2,0,1)).astype(np.float32)\n",
    "    image = torch.tensor(image, dtype=torch.float).to(device)\n",
    "    image = torch.unsqueeze(image, 0)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        model_6.eval()\n",
    "        output = model_6(image.to(device))\n",
    "\n",
    "    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in output]\n",
    "\n",
    "    boxes = outputs[0]['boxes'].data.numpy()\n",
    "    scores = outputs[0]['scores'].data.numpy()\n",
    "    \n",
    "    boxes = boxes[scores >= 0.8].astype(np.int32)\n",
    "    draw_boxes = boxes.copy()\n",
    "    pred_classes = [i for i in outputs[0]['labels'].cpu().numpy()]\n",
    "    \n",
    "    for k, box in enumerate(draw_boxes):\n",
    "        cv2.rectangle(orig_image,\n",
    "                    (int(box[0]), int(box[1])),\n",
    "                    (int(box[2]), int(box[3])),\n",
    "                    (0,0,255), 2)\n",
    "        cv2.putText(orig_image, str(pred_classes[k]),\n",
    "                    (int(box[0]), int(box[1]-5)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0),\n",
    "                    2, lineType=cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('Prediction', orig_image)\n",
    "        cv2.waitKey(10000)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving / loading\n",
    "# MODEL_PATH = pathlib.Path(\"Models\")\n",
    "# MODEL_NAME = \"03_ODTC.pt\"\n",
    "# MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "# torch.save(model_6.state_dict(), MODEL_SAVE_PATH)\n",
    "\n",
    "# path_rel = r'C:\\Users\\earle\\PythonMLenv\\env\\projects\\Models\\03_TFC.pt'\n",
    "# model_5.load_state_dict(torch.load(path_rel))\n",
    "# model_5.eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8174828e92d9e3a5ca64f417386608000b35cfcceb5edd6aed1e8771c000af37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
