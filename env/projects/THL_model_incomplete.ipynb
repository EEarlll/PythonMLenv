{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic python and ML Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib \n",
    "import re\n",
    "\n",
    "# matplotlib & others for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "\n",
    "# torchvision libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# misc\n",
    "from tqdm import tqdm\n",
    "from evaluate import load\n",
    "from transformers import AdamW\n",
    "from wordsegment import load, segment\n",
    "\n",
    "# model\n",
    "from strhub.data.module import SceneTextDataModule\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "working_dir = r'C:\\Users\\earle\\PythonMLenv\\env'\n",
    "os.getcwd()\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def ToDf(ostlistpath , label):\n",
    "    file_name = []\n",
    "    text = []\n",
    "    for i in ostlistpath:\n",
    "        file_name.append(i)\n",
    "        with open(label + i[:-4] + '.txt') as f:\n",
    "            line = [line for line in f][0]\n",
    "            text.append(line)\n",
    "\n",
    "    d = {'file_name': file_name, 'text' : text}\n",
    "    df = pd.DataFrame(d)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parseq torchhub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\earle/.cache\\torch\\hub\\baudm_parseq_main\n"
     ]
    }
   ],
   "source": [
    "# git clone https://github.com/baudm/parseq\n",
    "# os.chdir(r'C:\\Users\\earle\\PythonMLenv\\parseq')\n",
    "train_img_dir = r'C:\\Users\\earle\\PythonMLenv\\env\\projects\\data\\Higher_lower\\\\train\\\\images\\\\'\n",
    "train_img = [train_img_dir + i for i in os.listdir(train_img_dir)]\n",
    "parseq = torch.hub.load('baudm/parseq', 'parseq', pretrained= True).eval()\n",
    "img_transform = SceneTextDataModule.get_transform(parseq.hparams.img_size)\n",
    "\n",
    "w = []\n",
    "\n",
    "for i in train_img:\n",
    "    img = Image.open(i).convert(\"RGB\")\n",
    "    #display(img)\n",
    "    img = img_transform(img).unsqueeze(0)\n",
    "    logits = parseq(img)\n",
    "    pred = logits.softmax(-1)\n",
    "    label, confidence = parseq.tokenizer.decode(pred)\n",
    "    #print(label[0])\n",
    "    w.append(label[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [' '.join(segment(i)) for i in w]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # setups\n",
    "# train_img_dir = r'C:\\Users\\earle\\PythonMLenv\\env\\projects\\data\\Higher_lower\\\\train\\\\images\\\\'\n",
    "# test_img_dir = r'C:\\Users\\earle\\PythonMLenv\\env\\projects\\data\\Higher_lower\\\\test\\\\images\\\\'\n",
    "# train_label_dir = r'C:\\Users\\earle\\PythonMLenv\\env\\projects\\data\\Higher_lower\\\\train\\\\label\\\\'\n",
    "# test_label_dir = r'C:\\Users\\earle\\PythonMLenv\\env\\projects\\data\\Higher_lower\\\\test\\\\label\\\\'\n",
    "# plist = os.listdir(train_img_dir)\n",
    "# dict_train = {}\n",
    "# dict_test = {}\n",
    "\n",
    "# # for i in plist:\n",
    "# #     file_name = i[:-4]\n",
    "# #     image = Image.open(test_img_dir + '\\\\' + i)\n",
    "# #     display(image)\n",
    "# #     with open(test_label_dir + file_name + '.txt', 'w') as file:\n",
    "# #         image_name_input = input(\"Enter image text: \")\n",
    "# #         file.write(image_name_input)\n",
    "\n",
    "# train_df = ToDf(plist, train_label_dir)\n",
    "# test_df = ToDf(os.listdir(test_img_dir), test_label_dir)\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataset & dataloader\n",
    "\n",
    "# class HigherLowerDataset(Dataset):\n",
    "#     def __init__(self, root_dir, df, processor, max_target_length =128) -> None:\n",
    "#         self.root_dir = root_dir\n",
    "#         self.df = df\n",
    "#         self.processor = processor\n",
    "#         self.max_target_length = max_target_length\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         file_name = self.df['file_name'][index]\n",
    "#         text = self.df['text'][index]\n",
    "#         image = Image.open(self.root_dir + file_name).convert(\"RGB\")\n",
    "        \n",
    "#         pixel_values = self.processor(image, return_tensors = 'pt').pixel_values\n",
    "#         labels = self.processor.tokenizer(text , padding = 'max_length', max_length = self.max_target_length).input_ids\n",
    "#         labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "        \n",
    "#         encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "#         return encoding\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "        \n",
    "# processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "# train_dataset = HigherLowerDataset(root_dir=train_img_dir, df = train_df, processor=processor)\n",
    "# test_dataset = HigherLowerDataset(root_dir=test_img_dir, df = test_df, processor=processor)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=8 , shuffle=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "# model.to(device)\n",
    "# model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "# model.config.pad_token_id = processor.tokenizer.pad_token_type_id\n",
    "# model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# # set beam search parameters\n",
    "# model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "# model.config.max_length = 64\n",
    "# model.config.early_stopping = True\n",
    "# model.config.no_repeat_ngram_size = 3\n",
    "# model.config.length_penalty = 2.0\n",
    "# model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cer_metric = load(\"cer\")\n",
    "\n",
    "# def compute_cer(pred):\n",
    "#     labels_ids = pred.label_ids\n",
    "#     pred_ids = pred.predictions\n",
    "\n",
    "#     pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "#     labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "#     label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "#     cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "#     return {\"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train \n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     predict_with_generate=True,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     per_device_eval_batch_size=1,\n",
    "#     per_device_train_batch_size=1,\n",
    "#     fp16=True,\n",
    "#     output_dir=\"./\",\n",
    "#     logging_steps=2,\n",
    "#     save_steps=1000,\n",
    "#     eval_steps=200,\n",
    "# )\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     model = model,\n",
    "#     tokenizer = processor.feature_extractor,\n",
    "#     args = training_args,\n",
    "#     compute_metrics=compute_cer,\n",
    "#     train_dataset= train_dataset,\n",
    "#     eval_dataset= test_dataset,\n",
    "#     data_collator=default_data_collator\n",
    "# )\n",
    "# trainer.train()\n",
    "# trainer.save_model(r'C:\\Users\\earle\\PythonMLenv\\env\\projects\\Models')\n",
    "\n",
    "# train model\n",
    "optimizer = AdamW(model.parameters(), lr = 5e-5)\n",
    "optimizer.zero_grad()\n",
    "coutn = 0\n",
    "for i in range(3):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        coutn +=1\n",
    "        for k,v in batch.items():\n",
    "            batch[k] = v.to(device)\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        if (coutn+1) % 10 ==0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Loss: {train_loss/len(train_dataloader)}\")\n",
    "\n",
    "    model.eval()\n",
    "    valid_cer = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(test_dataloader):\n",
    "            outputs = model.generate(batch[\"pixel_values\"]).to(device)\n",
    "            cer = compute_cer(outputs, batch[\"labels\"])\n",
    "            valid_cer +=cer\n",
    "        print(\"Test CER: \", valid_cer/len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "# processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "\n",
    "# val_dir = r'C:\\Users\\earle\\PythonMLenv\\env\\projects\\data\\Higher_lower\\val\\images'\n",
    "# pjoined = [val_dir + \"\\\\\" +i for i in os.listdir(val_dir)]\n",
    "\n",
    "# for i in pjoined:\n",
    "#     image = Image.open(i).convert(\"RGB\")\n",
    "#     display(image)\n",
    "#     pixel_values = processor(image, return_tensors = \"pt\").pixel_values\n",
    "#     generated_ids = model.generate(pixel_values)\n",
    "#     generated_text = processor.batch_decode(generated_ids,skip_special_tokens = True)[0]\n",
    "#     print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8174828e92d9e3a5ca64f417386608000b35cfcceb5edd6aed1e8771c000af37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
